{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<P align=\"right\"> Andrew Koren </P>\n",
    "\n",
    "# Mth 416 H3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy is defined as\n",
    "\n",
    "$$\n",
    "H_b(p) = \\sum_{s\\in S} p_s \\log_b\\left(\\frac{1}{p_s}\\right) \\\\\n",
    "$$\n",
    "\n",
    "Since no base is stated, we'll assume $b=2$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.295461844238322\n",
      "1.4854752972273344\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "\n",
    "first_p = (0.6, 0.3, 0.1)\n",
    "second_p = (0.5, 0.3, 0.2)\n",
    "base = 2\n",
    "\n",
    "def get_entropy(probabilites, base):\n",
    "    entropy = sum([\n",
    "        p*log(1/p, base) for p in probabilites\n",
    "        ])\n",
    "    return entropy\n",
    "\n",
    "print(get_entropy(first_p, base))\n",
    "print(get_entropy(second_p, base))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second probability distribution has higher entropy ($1.49$ vs $1.30$). According to theorem $3.4.2$, we have $H_b(p) \\leq \\log_b m$ where $m = |S|$, with equality when $p$ is an equal distribution $(1/3, 1/3, 1/3)$. Thus the maximum entropy is $\\log_2(3) = 1.58$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5849625007211563"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log(3, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"page-break-after: always\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Shannon-Fano code,\n",
    "\n",
    "$y_s=\\left\\lceil \\log_b \\frac{1}{p_s} \\right\\rceil$\n",
    "\n",
    "For the distribution $(0.5, 0.3, 0.2)$, the SF code yeilds the lengths $(1, 2, 3)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "from math import ceil\n",
    "\n",
    "print([ceil(log(1/p, 2)) for p in second_p])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be mapped to the binary values [0, 10, 110]. We can check if it is optimal by checking if it is equivalent to the huffman code (same $\\sum_i p_iy_i$). We make the huffman code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5 0.3 0.2]\n",
      " [0.5 0.5 0. ]\n",
      " [0.  1.  0. ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['0.5', '0'],\n",
       "       ['0.3', '11'],\n",
       "       ['0.2', '10']], dtype='<U32')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from other_code import make_huffman, path_to_binary\n",
    "\n",
    "huffman, path = make_huffman(second_p)\n",
    "print(huffman)\n",
    "path_to_binary(path, second_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first matrix shows the tree method used to create the huffman code, and the second shows what codeword each probability is mapped to using the tree method. Since the huffman code is optimal and the SF code is the same with one codeword being one symbol longer, it is longer than an optimal code and thus not optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"page-break-after: always\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running the Huffman code algorithm on this code we can find the average codeword length of the Huffman code, which is an optimal binary UD code.\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\alpha & \\beta & \\gamma \\\\\n",
    "\\alpha & \\beta + \\gamma & 0 \\\\\n",
    "0 & \\alpha + \\beta + \\gamma & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Where $\\alpha + \\beta + \\gamma = 1$. <br> This yields $\\alpha \\rightarrow 0, \\beta \\rightarrow 10, \\gamma \\rightarrow 11$. \n",
    "\n",
    "Thus we can find the average codeword length $L$:\n",
    "$$\n",
    "L = \\alpha*1 + \\beta*2 + \\gamma*2 \\\\\n",
    "= \\alpha + 2(\\beta + \\gamma) = \\alpha + 2(1-\\alpha) = \\alpha + 2-2\\alpha = 2-\\alpha\n",
    "$$\n",
    "Thus the average codeword length is confirmed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"page-break-after: always\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use our algorithm from class to find the huffman code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4  0.3  0.1  0.1  0.06 0.04]\n",
      " [0.4  0.3  0.1  0.1  0.1  0.  ]\n",
      " [0.4  0.3  0.   0.2  0.1  0.  ]\n",
      " [0.4  0.3  0.   0.3  0.   0.  ]\n",
      " [0.4  0.   0.   0.6  0.   0.  ]\n",
      " [0.   0.   0.   1.   0.   0.  ]]\n",
      "[(5, 4), (2, 3), (4, 3), (1, 3), (0, 3)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['0.4', '0'],\n",
       "       ['0.3', '10'],\n",
       "       ['0.1', '1110'],\n",
       "       ['0.1', '1111'],\n",
       "       ['0.06', '1101'],\n",
       "       ['0.04', '1100']], dtype='<U32')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from other_code import make_huffman, path_to_binary\n",
    "\n",
    "weights = (0.4, 0.3, 0.1, 0.1, 0.06, 0.04)\n",
    "\n",
    "huffman_reduction, path = make_huffman(weights)\n",
    "\n",
    "#shows the steps\n",
    "print(huffman_reduction)\n",
    "print(path)\n",
    "\n",
    "path_to_binary(path, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The array above shows the reduction used to create the Huffman code, and the reduction path is used to make the corresponding optimal code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"page-break-after: always\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: $L(0.5, 0.3, 0.2) = 0.5 * 1 + 0.3 *2 + 0.2 *2 = 1.5\\rightarrow \\tilde p(e)_1 = + \\tilde p(e)_2 = 0.5 + 1 = 1.5$\n",
    "\n",
    "Proof:\n",
    "\n",
    "Applying **H1** combines two codewords of length $l$ and probability $p_d$ and $p_e$ into one of length $l-1$ and probability $p_d+p_e$. This changes $L$ to $L'$ by\n",
    "\n",
    "$$L = a + l*(p_e+p_d) \\rightarrow L' = a + (l-1)(p_e+p_d) = L - (p_e+p_d) = L - \\tilde p(e)$$\n",
    "where $a$ represents all unaffected terms. Note that $p_e + p_d = \\tilde p(e)$. After $|p|-1$ iterations, where $|p|$ is the number of probabilities in the distribution, there is only one probability left $\\tilde p(e) = 1$, where the only codeword left is the emptyset, $L= 1*0=0$. \n",
    "\n",
    "As such, **H1** iteratively subtracts $\\tilde p(e)$ from $L$ until $L=0$ to create the Huffman code. In reverse, this is the same as $$\\sum_i \\tilde p_i(e) = L$$ for each $\\tilde p_i(e)$ introduced by application of the Huffman rule **H1**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<!-- PFor a probability distribution of with $|p|$ entries, we show that $L = \\sum_{i=1}^{|p|} \\tilde p_i$ where each $\\tilde p_i$ is a new probability formed by **H1**\n",
    "\n",
    "$$\n",
    "L = p_1 y_1 + p_2 y_2 + \\dots + p_{|p|} y_{|p|}\n",
    "$$\n",
    "\n",
    "For $|p| = 2$ this is trivial, since $y_i = 1$\n",
    "\n",
    "$$\n",
    "p = (\\alpha, \\beta)\n",
    "L = \\alpha * 1 + \\beta *1 = 1\\\\\n",
    "\\tilde p = \\alpha + \\beta = 1\n",
    "$$ -->\n",
    "\n",
    "<!-- \n",
    "Moving backwards, **H1** splits a probability associated with a codeword length $l$ in two and assigns codewords of length $l+1$ to each. -->\n",
    "\n",
    "\n",
    "\n",
    "<!-- Proof: average length $L= \\sum_i p_i y_i = \\sum_i \\tilde p(e_i)$, where $e_i$ is the $i$-th iteration of **H1**. Using an optimal huffman code for some distribution $p = (p_1, \\dots, p_n)$ there is at least one $p_e = \\min(p)$ with associated minimum $y_e|e\\in{1,\\dots n}$. -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a). Note that $\\tilde p_s$ is the new probability distribution and $\\tilde c$ is the new code, each with one fewer elements than $p_s$ and $c$\n",
    "\n",
    "Since $|c(s)| = |\\tilde c(s)|$ or $|\\tilde c(s)|+1$ for $s$ corresponding to the lowest 2 probability symbols (longest length codewords), represented as $d,e$, let's conside a code with 1, 2, or 3 codewords of lenght $M(\\tilde c)$\n",
    "\n",
    "One codeword of length $M(\\tilde c)$: \n",
    "\n",
    "$$\n",
    "c(e) = \\tilde c(e)1 \\\\\n",
    "y_e = \\tilde y_e+1\n",
    "$$\n",
    "Since $\\tilde y_e$ is the longest codeword, $M(\\tilde c) = y_e$, and $M(c) = y_e+1$ (all other codewords are shorter and do not affect $M(c)$). Thus $M(c) = M(\\tilde c) + 1$\n",
    "\n",
    "This problem is incomplete. Sorry!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"page-break-after: always\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Since $p^{(l)}(0) + p^{(l)}(1) = 1$, (ii) implies $p^{(l)}(0) = p^{(l)}(1) = 0.5$.\n",
    "$$\n",
    "2p^{(l)}(0) = 1 \\rightarrow p^{(l)}(0)=1/2 \\rightarrow p^{(l)}(1) = 1/2\n",
    "$$\n",
    "\n",
    "<!-- However, since\n",
    "$P(000) = \\frac{1}{12} \\neq 1/2*1/2*1/2 = 1/8$, it's clear;y not memoryless. We'll get there in part (e).\n",
    " -->\n",
    "\n",
    "(b) Independence $\\rightarrow p^{(l_1)} \\otimes p^{(l_2)} = p^{l_1+l_2} = p^{(l_1, l_2)}$\n",
    "\n",
    "$$\n",
    "p^{(l_1)}(0) = p^{(l_1)}(1) = 1/2 \\\\\n",
    "p^{(l_2)}(0) = p^{(l_2)}(1) = 1/2 \\\\\n",
    "p^{(l_1)}(0) \\otimes p^{(l_2)}(0) = 1/2 * 1/2 = 1/4 = p^{(l_1, l_2)}(0,0) \\\\\n",
    "p^{(l_1, l_2)}(ab) = 1/4 | ab \\in \\mathbb{B}^2 \n",
    "$$\n",
    "\n",
    "(c) $P(a) = \\sum_{s\\in S} P(as) \\rightarrow P(00) = P(001) + P(000)$ from definition of $p^l(x)$\n",
    "\n",
    "Since $P(00)=1/4$ and $P(000) = 1/12$, $P(001) = P(00)-P(000) = 1/6$\n",
    "\n",
    "This can be repeated for each restriction with two $0$ terms\n",
    "\n",
    "$$\n",
    "p^{(1, 3)}(00) = P(000) + P(010) = 1/4 \\rightarrow P(010) = 1/6 \\\\\n",
    "P^{(2,3)}(00) = P(000) + P(100) = 1/4 \\rightarrow P(100) = 1/6 \n",
    "$$\n",
    "\n",
    "Next check the values of even terms\n",
    "\n",
    "$$\n",
    "p^{(1, 2)}(10) = P(100) + P(101) = 1/4 \\\\\n",
    "P(101) = 1/4 - P(100) = 1/4 - 1/6 = 1/12 \\\\\n",
    "p^{2,3}(10) = P(010) + P(110) \\rightarrow P(110) = 1/12 \\\\\n",
    "p^{(1,2)}(01) = P(010) + P(011) \\rightarrow P(011) = 1/12\n",
    "$$\n",
    "\n",
    "The final term $P(111)$ is $1$ minus all other probabilities\n",
    "$$\n",
    "P(111) = 1-\\left(4*\\frac{1}{12}+3*\\frac{1}{6}\\right) = \\frac{1}{6}\n",
    "$$\n",
    "\n",
    "This confirms each case and shows part (c) is true.\n",
    "\n",
    "(d) Independence $\\rightarrow p^{(1)} \\otimes p^{(2,3)} = p^{(1, 2, 3)} \\stackrel{?}{=} p^3$\n",
    "\n",
    "Let's show an example where this is false using $P(000)=1/12$.\n",
    "\n",
    "$$\n",
    "p^{(1)}(0) = 1/2 \\\\\n",
    "p^{(2,3)}(00) = 1/4 \\\\\n",
    "\n",
    "p^{(1)} \\otimes p^{(2,3)} = 1/8 \\neq 1/12 = p^3(000)\n",
    "$$\n",
    "\n",
    "As such, they are not independent with respect to $p^3$\n",
    "\n",
    "(e) Memoryless if $P(as) = P(a)P(s)$. \n",
    "\n",
    "Lemma 4.3.3(a): $p^{l+r} = p^l \\otimes p^r$\n",
    "\n",
    "From part (a) and (b): $p^1(0)=1/2$, $p^2(00) = 1/4$. By lemma 4.3.3, a memoryless source should have $p^{(2+1)}(000) = p^1(0)p^2(00) = 1/8$, but instead we have $p^3(000) = P(000) = 1/12 \\neq 1/8 = p^1(0)p^2(00)$, so the source is not memoryless"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
